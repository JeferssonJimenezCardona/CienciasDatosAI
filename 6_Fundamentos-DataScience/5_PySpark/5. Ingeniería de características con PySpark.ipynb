{"cells":[{"cell_type":"markdown","metadata":{"id":"tlfhyC6kC3av"},"source":["# Ingeniería de características con PySpark"]},{"cell_type":"code","source":["!pip install findspark\n","!pip install pyspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RmcSFBJxDHoZ","executionInfo":{"status":"ok","timestamp":1667421273360,"user_tz":300,"elapsed":57939,"user":{"displayName":"Jefersson Jimenez","userId":"10027767004537686734"}},"outputId":"da109109-1f4f-4e66-8f92-b5688a6a9a5d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting findspark\n","  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n","Installing collected packages: findspark\n","Successfully installed findspark-2.0.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyspark\n","  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n","\u001b[K     |████████████████████████████████| 281.4 MB 47 kB/s \n","\u001b[?25hCollecting py4j==0.10.9.5\n","  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n","\u001b[K     |████████████████████████████████| 199 kB 44.5 MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845513 sha256=8e8898750cff3ceb225164634661857839b47479dd43c59925de58bb57949f87\n","  Stored in directory: /root/.cache/pip/wheels/42/59/f5/79a5bf931714dcd201b26025347785f087370a10a3329a899c\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"JpTdrBlZC3az","executionInfo":{"status":"ok","timestamp":1667421755669,"user_tz":300,"elapsed":8,"user":{"displayName":"Jefersson Jimenez","userId":"10027767004537686734"}}},"outputs":[],"source":["import findspark\n","findspark.init()"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3t_JRjntC3a0","executionInfo":{"status":"ok","timestamp":1667421767135,"user_tz":300,"elapsed":10292,"user":{"displayName":"Jefersson Jimenez","userId":"10027767004537686734"}},"outputId":"83b7bdc1-da3e-4ce8-d16c-750119beecd1"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py:114: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n","  FutureWarning,\n"]}],"source":["# Initializing a Spark session\n","from pyspark.sql import SparkSession\n","\n","from pyspark.sql import SQLContext\n","\n","spark = SparkSession.builder.appName('feature-engine').getOrCreate()\n","\n","sqlContext = SQLContext(spark)"]},{"cell_type":"markdown","metadata":{"id":"HOVu_TQzC3a1"},"source":["# Transformación de variables"]},{"cell_type":"markdown","metadata":{"id":"kubAKjcyC3a1"},"source":["### Vector assembler\n","\n","VectorAssembler es un transformador que combina una lista determinada de columnas en una sola columna de vector. Es útil para combinar características sin procesar y características generadas por diferentes transformadores de características en un solo vector de características, con el fin de entrenar modelos ML "]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5YQ8gz2IC3a2","executionInfo":{"status":"ok","timestamp":1667422229145,"user_tz":300,"elapsed":10151,"user":{"displayName":"Jefersson Jimenez","userId":"10027767004537686734"}},"outputId":"e94451f8-d18b-4563-a89e-0637e9fbdc07"},"outputs":[{"output_type":"stream","name":"stdout","text":["Assembled columns 'hour', 'mobile',       'userFeatures' to vector column 'features'\n","+-----------------------+-------+\n","|features               |clicked|\n","+-----------------------+-------+\n","|[18.0,1.0,0.0,10.0,0.5]|1.0    |\n","+-----------------------+-------+\n","\n"]}],"source":["from pyspark.ml.linalg import Vectors\n","from pyspark.ml.feature import VectorAssembler\n","\n","dataset = spark.createDataFrame(\n","    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)],\n","    [\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\"])\n","\n","assembler = VectorAssembler(\n","    inputCols=[\"hour\", \"mobile\", \"userFeatures\"],\n","    outputCol=\"features\")\n","\n","output = assembler.transform(dataset)\n","\n","print(\"Assembled columns 'hour', 'mobile', \\\n","      'userFeatures' to vector column 'features'\")\n","      \n","      \n","output.select(\"features\", \"clicked\").show(truncate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ou1Qbiy2C3a6"},"outputs":[],"source":["# Let us import the vector assembler\n","from pyspark.ml.feature import VectorAssembler\n","# Once the Vector assembler is imported we are required to create the object of the same. Here I will create an object anmed va\n","# The above result shows that we have three features in 'FakeIntDF' i.e. int1, int2, int3. Let us create the object va so as to combine the three features into a single column named features\n","assembler = VectorAssembler(inputCols=[\"int1\", \"int2\", \"int3\"],outputCol=\"features\")\n","# Now let us use the transform method to transform our dataset\n","assembler.transform(fakeIntDF).show()"]},{"cell_type":"markdown","metadata":{"id":"mDO_1WRfC3a7"},"source":["## Variables Numéricas"]},{"cell_type":"markdown","metadata":{"id":"0FIl0XZbC3a8"},"source":["### Bucketing\n","Bucketing (agrupamiento) es el enfoque más sencillo para convertir las variables continuas en variables categóricas. En pyspark, la tarea de agrupamiento se puede realizar fácilmente utilizando la clase Bucketizer.\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jw-ukaxPC3a9","executionInfo":{"status":"ok","timestamp":1667422244499,"user_tz":300,"elapsed":786,"user":{"displayName":"Jefersson Jimenez","userId":"10027767004537686734"}},"outputId":"fa075c1a-ed6e-469d-b5d9-5b2a957d745b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Bucketizer output with 4 buckets\n","+--------+----------------+\n","|features|bucketedFeatures|\n","+--------+----------------+\n","|  -999.9|             0.0|\n","|    -0.5|             1.0|\n","|    -0.3|             1.0|\n","|     0.0|             2.0|\n","|     0.2|             2.0|\n","|   999.9|             3.0|\n","+--------+----------------+\n","\n"]}],"source":["from pyspark.ml.feature import Bucketizer\n","\n","splits = [-float(\"inf\"), -0.5, 0.0, 0.5, float(\"inf\")]\n","\n","data = [(-999.9,), (-0.5,), (-0.3,), (0.0,), (0.2,), (999.9,)]\n","dataFrame = spark.createDataFrame(data, [\"features\"])\n","\n","bucketizer = Bucketizer(splits=splits, \n","                        inputCol=\"features\", \n","                        outputCol=\"bucketedFeatures\")\n","\n","# Transform original data into its bucket index.\n","bucketedData = bucketizer.transform(dataFrame)\n","\n","print(\"Bucketizer output with %d buckets\" % (len(bucketizer.getSplits())-1))\n","bucketedData.show()"]},{"cell_type":"markdown","metadata":{"id":"MXe1YnVPC3a-"},"source":["## Escalado y Normalización\n","\n","El escalado y la normalización es otra tarea común con la que nos encontramos al manejar variables continuas. No siempre es imperativo escalar y normalizar las características. Sin embargo, es muy recomendable escalar y normalizar las características antes de aplicar un algoritmo ML para evitar el riesgo de que un algoritmo sea insensible a ciertas características.\n","\n","Spark ML nos proporciona una clase \"StandardScaler\" para facilitar el escalado y la normalización de funciones."]},{"cell_type":"markdown","metadata":{"id":"BUdaJjcqC3a_"},"source":["### StandardScaler"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":381},"id":"r6ELnd5kC3a_","executionInfo":{"status":"error","timestamp":1667422250374,"user_tz":300,"elapsed":919,"user":{"displayName":"Jefersson Jimenez","userId":"10027767004537686734"}},"outputId":"f692b0d3-11f5-49a1-868e-44bb1d3709c4"},"outputs":[{"output_type":"error","ename":"AnalysisException","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-c7f6280b47ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataFrame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"libsvm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/sample_libsvm_data.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n\u001b[1;32m      5\u001b[0m                         withStd=True, withMean=False)\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/content/data/sample_libsvm_data.txt"]}],"source":["\n","from pyspark.ml.feature import StandardScaler\n","\n","dataFrame = spark.read.format(\"libsvm\").load(\"data/sample_libsvm_data.txt\")\n","scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n","                        withStd=True, withMean=False)\n","\n","# Compute summary statistics by fitting the StandardScaler\n","scalerModel = scaler.fit(dataFrame)\n","\n","# Normalize each feature to have unit standard deviation.\n","scaledData = scalerModel.transform(dataFrame)\n","scaledData.show()"]},{"cell_type":"markdown","metadata":{"id":"3DIKOxyoC3bA"},"source":["### RobustScaler"]},{"cell_type":"markdown","metadata":{"id":"WWk5gtNmC3bA"},"source":["RobustScalertransforma un conjunto de datos numéricos, eliminando la mediana y escalando los datos de acuerdo con un rango de cuantiles específico. Su comportamiento es bastante similar a StandardScaler, sin embargo, se utilizan la mediana y el rango de cuantiles en lugar de la media y la desviación estándar, lo que lo hace **robusto a los valores atípicos**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pmSvq-r1C3bA","outputId":"0af20baa-b18a-4aa8-ff5f-d43d8851a7d5"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----+--------------------+--------------------+\n","|label|            features|      scaledFeatures|\n","+-----+--------------------+--------------------+\n","|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n","|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n","|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n","|  1.0|(692,[152,153,154...|(692,[152,153,154...|\n","|  1.0|(692,[151,152,153...|(692,[151,152,153...|\n","|  0.0|(692,[129,130,131...|(692,[129,130,131...|\n","|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n","|  1.0|(692,[99,100,101,...|(692,[99,100,101,...|\n","|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n","|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n","|  1.0|(692,[154,155,156...|(692,[154,155,156...|\n","|  0.0|(692,[153,154,155...|(692,[153,154,155...|\n","|  0.0|(692,[151,152,153...|(692,[151,152,153...|\n","|  1.0|(692,[129,130,131...|(692,[129,130,131...|\n","|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n","|  1.0|(692,[150,151,152...|(692,[150,151,152...|\n","|  0.0|(692,[124,125,126...|(692,[124,125,126...|\n","|  0.0|(692,[152,153,154...|(692,[152,153,154...|\n","|  1.0|(692,[97,98,99,12...|(692,[97,98,99,12...|\n","|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n","+-----+--------------------+--------------------+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark.ml.feature import RobustScaler\n","\n","dataFrame = spark.read.format(\"libsvm\").load(\"data/sample_libsvm_data.txt\")\n","\n","scaler = RobustScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n","                      withScaling=True, withCentering=False,\n","                      lower=0.25, upper=0.75)\n","\n","# Compute summary statistics by fitting the RobustScaler\n","scalerModel = scaler.fit(dataFrame)\n","\n","# Transform each feature to have unit quantile range.\n","scaledData = scalerModel.transform(dataFrame)\n","scaledData.show()"]},{"cell_type":"markdown","metadata":{"id":"TXIusIkAC3bB"},"source":["### MinMaxScaler\n","\n","StandardScaler estandariza las características con una media cero y una desviación estándar de 1. A veces, nos encontramos con situaciones en las que necesitamos escalar valores dentro de un rango determinado (es decir, máximo y mínimo). Para tal tarea, Spark ML proporciona un MinMaxScaler.\n","\n","StandardScaler y MinMaxScaler son muy similares, la única diferencia es que podemos proporcionar el valor mínimo y los valores máximos dentro de los cuales deseamos escalar las características.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rLKvqOrEC3bB","outputId":"9c1bc2da-9ad9-4f7e-b5d2-c1989e54551b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Features scaled to range: [0.000000, 1.000000]\n","+--------------+--------------+\n","|      features|scaledFeatures|\n","+--------------+--------------+\n","|[1.0,0.1,-1.0]|     (3,[],[])|\n","| [2.0,1.1,1.0]| [0.5,0.1,0.5]|\n","|[3.0,10.1,3.0]| [1.0,1.0,1.0]|\n","+--------------+--------------+\n","\n"]}],"source":["from pyspark.ml.feature import MinMaxScaler\n","from pyspark.ml.linalg import Vectors\n","\n","dataFrame = spark.createDataFrame([\n","    (0, Vectors.dense([1.0, 0.1, -1.0]),),\n","    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n","    (2, Vectors.dense([3.0, 10.1, 3.0]),)\n","], [\"id\", \"features\"])\n","\n","scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n","\n","# Compute summary statistics and generate MinMaxScalerModel\n","scalerModel = scaler.fit(dataFrame)\n","\n","# rescale each feature to range [min, max].\n","scaledData = scalerModel.transform(dataFrame)\n","print(\"Features scaled to range: [%f, %f]\" % (scaler.getMin(), scaler.getMax()))\n","scaledData.select(\"features\", \"scaledFeatures\").show()"]},{"cell_type":"markdown","metadata":{"id":"RlirTIO6C3bB"},"source":["### MaxAbsScaler\n","\n","MaxAbsScaler transforma un conjunto de datos, reescalando cada característica al rango [-1, 1], dividiendo entre el valor absoluto máximo en cada característica. No desplaza los datos y, por lo tanto, **no destruye ninguna escasez**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D-qPotUiC3bB","outputId":"a806bfcb-db77-4043-868d-69579ce1a758"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------+--------------------+\n","|      features|      scaledFeatures|\n","+--------------+--------------------+\n","|[1.0,0.1,-8.0]|[0.25,0.010000000...|\n","|[2.0,1.0,-4.0]|      [0.5,0.1,-0.5]|\n","|[4.0,10.0,8.0]|       [1.0,1.0,1.0]|\n","+--------------+--------------------+\n","\n"]}],"source":["from pyspark.ml.feature import MaxAbsScaler\n","from pyspark.ml.linalg import Vectors\n","\n","dataFrame = spark.createDataFrame([\n","    (0, Vectors.dense([1.0, 0.1, -8.0]),),\n","    (1, Vectors.dense([2.0, 1.0, -4.0]),),\n","    (2, Vectors.dense([4.0, 10.0, 8.0]),)\n","], [\"id\", \"features\"])\n","\n","scaler = MaxAbsScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n","\n","# Compute summary statistics and generate MaxAbsScalerModel\n","scalerModel = scaler.fit(dataFrame)\n","\n","# rescale each feature to range [-1, 1].\n","scaledData = scalerModel.transform(dataFrame)\n","\n","scaledData.select(\"features\", \"scaledFeatures\").show()"]},{"cell_type":"markdown","metadata":{"id":"kXKeLECkC3bC"},"source":["### Normalizer\n","\n","Es un transformados que normaliza los datos para tener una norma unitaria. Toma el parámetro p, que especifica la p-norma utilizada para la normalización. (p = 2 por defecto.) \n","Esta normalización puede ayudar a estandarizar los datos de entrada y mejorar el comportamiento de los algoritmos de aprendizaje.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"14cKn3XdC3bC","outputId":"7a89d163-dc05-42eb-aae3-a5d0276a7c1e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Normalized using L^1 norm\n","+---+--------------+------------------+\n","| id|      features|      normFeatures|\n","+---+--------------+------------------+\n","|  0|[1.0,0.5,-1.0]|    [0.4,0.2,-0.4]|\n","|  1| [2.0,1.0,1.0]|   [0.5,0.25,0.25]|\n","|  2|[4.0,10.0,2.0]|[0.25,0.625,0.125]|\n","+---+--------------+------------------+\n","\n","Normalized using L^inf norm\n","+---+--------------+--------------+\n","| id|      features|  normFeatures|\n","+---+--------------+--------------+\n","|  0|[1.0,0.5,-1.0]|[1.0,0.5,-1.0]|\n","|  1| [2.0,1.0,1.0]| [1.0,0.5,0.5]|\n","|  2|[4.0,10.0,2.0]| [0.4,1.0,0.2]|\n","+---+--------------+--------------+\n","\n"]}],"source":["from pyspark.ml.feature import Normalizer\n","from pyspark.ml.linalg import Vectors\n","\n","dataFrame = spark.createDataFrame([\n","    (0, Vectors.dense([1.0, 0.5, -1.0]),),\n","    (1, Vectors.dense([2.0, 1.0, 1.0]),),\n","    (2, Vectors.dense([4.0, 10.0, 2.0]),)\n","], [\"id\", \"features\"])\n","\n","# Normalize each Vector using $L^1$ norm.\n","normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\n","l1NormData = normalizer.transform(dataFrame)\n","print(\"Normalized using L^1 norm\")\n","l1NormData.show()\n","\n","# Normalize each Vector using $L^\\infty$ norm.\n","lInfNormData = normalizer.transform(dataFrame, {normalizer.p: float(\"inf\")})\n","print(\"Normalized using L^inf norm\")\n","lInfNormData.show()"]},{"cell_type":"markdown","metadata":{"id":"A8jcJ4q-C3bC"},"source":["## Trabajar con características categóricas\n","\n","La mayoría de los algoritmos ML requieren convertir características categóricas en numéricas.\n"]},{"cell_type":"markdown","metadata":{"id":"9vATupYUC3bD"},"source":["### StringIndexer \n","\n","Asigna valores numéricos a diferentes variables de texto. StringIndexercodifica una columna de cadenas de etiquetas en una columna de índices de etiquetas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5cLYM2n5C3bD","outputId":"7cc48c6c-d123-40b6-c97c-4aa2e061b71f"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+--------+-------------+\n","| id|category|categoryIndex|\n","+---+--------+-------------+\n","|  0|       a|          0.0|\n","|  1|       b|          2.0|\n","|  2|       c|          1.0|\n","|  3|       a|          0.0|\n","|  4|       a|          0.0|\n","|  5|       c|          1.0|\n","+---+--------+-------------+\n","\n"]}],"source":["from pyspark.ml.feature import StringIndexer\n","\n","df = spark.createDataFrame(\n","    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n","    [\"id\", \"category\"])\n","\n","indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n","indexed = indexer.fit(df).transform(df)\n","indexed.show()"]},{"cell_type":"markdown","metadata":{"id":"plWtu5yQC3bD"},"source":["### OneHotEncoder\n","\n","Es el tipo de transformación más común que se realiza durante el preprocesamiento. Mapea una característica categórica a un vector binario con un valor que indica la presencia de un valor de característica específico.\n","\n","Esta codificación permite que los algoritmos que esperan características continuas, como la regresión logística, utilicen características categóricas. Para datos de entrada de string, es común codificar características categóricas usando StringIndexer primero."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"APkqFgwiC3bD","outputId":"727dccf0-4281-4d2e-8cc7-a10416cc9cd6"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------+--------------+-------------+-------------+\n","|categoryIndex1|categoryIndex2| categoryVec1| categoryVec2|\n","+--------------+--------------+-------------+-------------+\n","|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n","|           1.0|           0.0|(2,[1],[1.0])|(2,[0],[1.0])|\n","|           2.0|           1.0|    (2,[],[])|(2,[1],[1.0])|\n","|           0.0|           2.0|(2,[0],[1.0])|    (2,[],[])|\n","|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n","|           2.0|           0.0|    (2,[],[])|(2,[0],[1.0])|\n","+--------------+--------------+-------------+-------------+\n","\n"]}],"source":["from pyspark.ml.feature import OneHotEncoder\n","\n","df = spark.createDataFrame([\n","    (0.0, 1.0),\n","    (1.0, 0.0),\n","    (2.0, 1.0),\n","    (0.0, 2.0),\n","    (0.0, 1.0),\n","    (2.0, 0.0)\n","], [\"categoryIndex1\", \"categoryIndex2\"])\n","\n","encoder = OneHotEncoder(inputCols=[\"categoryIndex1\", \"categoryIndex2\"],\n","                        outputCols=[\"categoryVec1\", \"categoryVec2\"])\n","model = encoder.fit(df)\n","encoded = model.transform(df)\n","encoded.show()"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"vQEZu1zXC3bE"},"source":["### IndexToString\n","\n","A veces nos encontramos con situaciones en las que es necesario volver a convertir **los valores indexados en texto**. Para hacer esto, Spark ML proporciona una clase IndextoString. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vJPGx8ZQC3bE","outputId":"cbd89475-8883-4993-858e-c2c9629d9407"},"outputs":[{"name":"stdout","output_type":"stream","text":["Transformed string column 'category' to indexed column 'categoryIndex'\n","+---+--------+-------------+\n","| id|category|categoryIndex|\n","+---+--------+-------------+\n","|  0|       a|          0.0|\n","|  1|       b|          2.0|\n","|  2|       c|          1.0|\n","|  3|       a|          0.0|\n","|  4|       a|          0.0|\n","|  5|       c|          1.0|\n","+---+--------+-------------+\n","\n","StringIndexer will store labels in output column metadata\n","\n","Transformed indexed column 'categoryIndex' back to original string column 'originalCategory' using labels in metadata\n","+---+-------------+----------------+\n","| id|categoryIndex|originalCategory|\n","+---+-------------+----------------+\n","|  0|          0.0|               a|\n","|  1|          2.0|               b|\n","|  2|          1.0|               c|\n","|  3|          0.0|               a|\n","|  4|          0.0|               a|\n","|  5|          1.0|               c|\n","+---+-------------+----------------+\n","\n"]}],"source":["from pyspark.ml.feature import IndexToString, StringIndexer\n","\n","df = spark.createDataFrame(\n","    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n","    [\"id\", \"category\"])\n","\n","indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n","model = indexer.fit(df)\n","indexed = model.transform(df)\n","\n","print(\"Transformed string column '%s' to indexed column '%s'\"\n","      % (indexer.getInputCol(), indexer.getOutputCol()))\n","indexed.show()\n","\n","print(\"StringIndexer will store labels in output column metadata\\n\")\n","\n","converter = IndexToString(inputCol=\"categoryIndex\", outputCol=\"originalCategory\")\n","converted = converter.transform(indexed)\n","\n","print(\"Transformed indexed column '%s' back to original string column '%s' using \"\n","      \"labels in metadata\" % (converter.getInputCol(), converter.getOutputCol()))\n","converted.select(\"id\", \"categoryIndex\", \"originalCategory\").show()"]},{"cell_type":"markdown","metadata":{"id":"lrvA5YU8C3bE"},"source":["### VectorIndexer\n","\n","VectorIndexer ayuda a indexar características **categóricas en conjuntos de datos de vectores**. Puede decidir automáticamente qué características son categóricas y convertir los valores originales en índices de categoría. \n","\n","En el siguiente ejemplo, lee un conjunto de datos de puntos etiquetados y luego usa VectorIndexer para decidir qué características deben tratarse como categóricas. Luego, transforma los valores de las características categóricas en sus índices. Estos datos transformados podrían luego pasarse a algoritmos como DecisionTreeRegressor que manejan características categóricas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NaDHGPyeC3bF","outputId":"6f149826-2195-41b6-a1a7-9fd059d93d7e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Chose 351 categorical features: 645, 69, 365, 138, 101, 479, 333, 249, 0, 555, 666, 88, 170, 115, 276, 308, 5, 449, 120, 247, 614, 677, 202, 10, 56, 533, 142, 500, 340, 670, 174, 42, 417, 24, 37, 25, 257, 389, 52, 14, 504, 110, 587, 619, 196, 559, 638, 20, 421, 46, 93, 284, 228, 448, 57, 78, 29, 475, 164, 591, 646, 253, 106, 121, 84, 480, 147, 280, 61, 221, 396, 89, 133, 116, 1, 507, 312, 74, 307, 452, 6, 248, 60, 117, 678, 529, 85, 201, 220, 366, 534, 102, 334, 28, 38, 561, 392, 70, 424, 192, 21, 137, 165, 33, 92, 229, 252, 197, 361, 65, 97, 665, 583, 285, 224, 650, 615, 9, 53, 169, 593, 141, 610, 420, 109, 256, 225, 339, 77, 193, 669, 476, 642, 637, 590, 679, 96, 393, 647, 173, 13, 41, 503, 134, 73, 105, 2, 508, 311, 558, 674, 530, 586, 618, 166, 32, 34, 148, 45, 161, 279, 64, 689, 17, 149, 584, 562, 176, 423, 191, 22, 44, 59, 118, 281, 27, 641, 71, 391, 12, 445, 54, 313, 611, 144, 49, 335, 86, 672, 172, 113, 681, 219, 419, 81, 230, 362, 451, 76, 7, 39, 649, 98, 616, 477, 367, 535, 103, 140, 621, 91, 66, 251, 668, 198, 108, 278, 223, 394, 306, 135, 563, 226, 3, 505, 80, 167, 35, 473, 675, 589, 162, 531, 680, 255, 648, 112, 617, 194, 145, 48, 557, 690, 63, 640, 18, 282, 95, 310, 50, 67, 199, 673, 16, 585, 502, 338, 643, 31, 336, 613, 11, 72, 175, 446, 612, 143, 43, 250, 231, 450, 99, 363, 556, 87, 203, 671, 688, 104, 368, 588, 40, 304, 26, 258, 390, 55, 114, 171, 139, 418, 23, 8, 75, 119, 58, 667, 478, 536, 82, 620, 447, 36, 168, 146, 30, 51, 190, 19, 422, 564, 305, 107, 4, 136, 506, 79, 195, 474, 664, 532, 94, 283, 395, 332, 528, 644, 47, 15, 163, 200, 68, 62, 277, 691, 501, 90, 111, 254, 227, 337, 122, 83, 309, 560, 639, 676, 222, 592, 364, 100\n","+-----+--------------------+--------------------+\n","|label|            features|             indexed|\n","+-----+--------------------+--------------------+\n","|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n","|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n","|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n","|  1.0|(692,[152,153,154...|(692,[152,153,154...|\n","|  1.0|(692,[151,152,153...|(692,[151,152,153...|\n","|  0.0|(692,[129,130,131...|(692,[129,130,131...|\n","|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n","|  1.0|(692,[99,100,101,...|(692,[99,100,101,...|\n","|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n","|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n","|  1.0|(692,[154,155,156...|(692,[154,155,156...|\n","|  0.0|(692,[153,154,155...|(692,[153,154,155...|\n","|  0.0|(692,[151,152,153...|(692,[151,152,153...|\n","|  1.0|(692,[129,130,131...|(692,[129,130,131...|\n","|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n","|  1.0|(692,[150,151,152...|(692,[150,151,152...|\n","|  0.0|(692,[124,125,126...|(692,[124,125,126...|\n","|  0.0|(692,[152,153,154...|(692,[152,153,154...|\n","|  1.0|(692,[97,98,99,12...|(692,[97,98,99,12...|\n","|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n","+-----+--------------------+--------------------+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark.ml.feature import VectorIndexer\n","\n","data = spark.read.format(\"libsvm\").load(\"data/sample_libsvm_data.txt\")\n","\n","indexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexed\", maxCategories=10)\n","indexerModel = indexer.fit(data)\n","\n","categoricalFeatures = indexerModel.categoryMaps\n","print(\"Chose %d categorical features: %s\" %\n","      (len(categoricalFeatures), \", \".join(str(k) for k in categoricalFeatures.keys())))\n","\n","# Create new column \"indexed\" with categorical values transformed to indices\n","indexedData = indexerModel.transform(data)\n","indexedData.show()"]},{"cell_type":"markdown","metadata":{"id":"d-xQnyQoC3bF"},"source":["# Imputación de variables faltantes"]},{"cell_type":"markdown","metadata":{"id":"ILOHOHg4C3bF"},"source":["### Imputer\n","\n","Completa los valores faltantes en un conjunto de datos, utilizando la media, la mediana o la moda de las columnas en las que se encuentran los valores faltantes. Las columnas de entrada deben ser de tipo numérico. Actualmente Imputer no admite características categóricas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zlm0eBR7C3bF","outputId":"34ca62e2-c924-47d5-844b-93d42b5a7096"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+---+-----+-----+\n","|  a|  b|out_a|out_b|\n","+---+---+-----+-----+\n","|1.0|NaN|  1.0|  4.0|\n","|2.0|NaN|  2.0|  4.0|\n","|NaN|3.0|  3.0|  3.0|\n","|4.0|4.0|  4.0|  4.0|\n","|5.0|5.0|  5.0|  5.0|\n","+---+---+-----+-----+\n","\n"]}],"source":["from pyspark.ml.feature import Imputer\n","\n","df = spark.createDataFrame([\n","    (1.0, float(\"nan\")),\n","    (2.0, float(\"nan\")),\n","    (float(\"nan\"), 3.0),\n","    (4.0, 4.0),\n","    (5.0, 5.0)\n","], [\"a\", \"b\"])\n","\n","imputer = Imputer(inputCols=[\"a\", \"b\"], outputCols=[\"out_a\", \"out_b\"])\n","model = imputer.fit(df)\n","\n","model.transform(df).show()"]},{"cell_type":"markdown","metadata":{"id":"GwNZlqjEC3bG"},"source":["# Selección de carácteristicas"]},{"cell_type":"markdown","metadata":{"id":"VhfIUqDqC3bG"},"source":["### VarianceThresholdSelector\n","\n","VarianceThresholdSelectores un selector que elimina las características de baja variación. varianceThresholdSe eliminará las características con una variación no mayor que la del varianceThreshold (0 por defecto).\n","\n","**Solo para versiones de Spark >= 3.2**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EvQl4d-KC3bG"},"outputs":[],"source":["from pyspark.ml.feature import VarianceThresholdSelector\n","from pyspark.ml.linalg import Vectors\n","\n","df = spark.createDataFrame([\n","    (1, Vectors.dense([6.0, 7.0, 0.0, 7.0, 6.0, 0.0])),\n","    (2, Vectors.dense([0.0, 9.0, 6.0, 0.0, 5.0, 9.0])),\n","    (3, Vectors.dense([0.0, 9.0, 3.0, 0.0, 5.0, 5.0])),\n","    (4, Vectors.dense([0.0, 9.0, 8.0, 5.0, 6.0, 4.0])),\n","    (5, Vectors.dense([8.0, 9.0, 6.0, 5.0, 4.0, 4.0])),\n","    (6, Vectors.dense([8.0, 9.0, 6.0, 0.0, 0.0, 0.0]))], [\"id\", \"features\"])\n","\n","selector = VarianceThresholdSelector(varianceThreshold=8.0, outputCol=\"selectedFeatures\")\n","\n","result = selector.fit(df).transform(df)\n","\n","print(\"Output: Features with variance lower than %f are removed.\" %\n","      selector.getVarianceThreshold())\n","result.show()"]},{"cell_type":"markdown","metadata":{"id":"YjOUHGs9C3bH"},"source":["### UnivariateFeatureSelector\n","\n","UnivariateFeatureSelectoropera en etiquetas categóricas / continuas con características categóricas / continuas. El usuario puede establecer featureTypey labelType, y Spark seleccionará la función de puntuación que se utilizará según el featureTypey especificado labelType.\n","\n","**Solo para versiones de Spark >= 3.2**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SIy4V9GpC3bI"},"outputs":[],"source":["from pyspark.ml.feature import UnivariateFeatureSelector\n","from pyspark.ml.linalg import Vectors\n","\n","df = spark.createDataFrame([\n","    (1, Vectors.dense([1.7, 4.4, 7.6, 5.8, 9.6, 2.3]), 3.0,),\n","    (2, Vectors.dense([8.8, 7.3, 5.7, 7.3, 2.2, 4.1]), 2.0,),\n","    (3, Vectors.dense([1.2, 9.5, 2.5, 3.1, 8.7, 2.5]), 3.0,),\n","    (4, Vectors.dense([3.7, 9.2, 6.1, 4.1, 7.5, 3.8]), 2.0,),\n","    (5, Vectors.dense([8.9, 5.2, 7.8, 8.3, 5.2, 3.0]), 4.0,),\n","    (6, Vectors.dense([7.9, 8.5, 9.2, 4.0, 9.4, 2.1]), 4.0,)], [\"id\", \"features\", \"label\"])\n","\n","\n","selector = UnivariateFeatureSelector(featuresCol=\"features\", outputCol=\"selectedFeatures\",\n","                                     labelCol=\"label\", selectionMode=\"numTopFeatures\")\n","\n","selector.setFeatureType(\"continuous\").setLabelType(\"categorical\").setSelectionThreshold(1)\n","\n","result = selector.fit(df).transform(df)\n","\n","print(\"UnivariateFeatureSelector output with top %d features selected using f_classif\"\n","      % selector.getSelectionThreshold())\n","result.show()"]},{"cell_type":"markdown","metadata":{"id":"lUKqKP69C3bJ"},"source":["### ChiSqSelector\n","\n","ChiSqSelector hace referencia a la selección de características mediante Chi-Squared. Opera sobre datos etiquetados con **características categóricas**. ChiSqSelector utiliza la prueba de independencia Chi-Squared para decidir qué características elegir."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"f3Ptp8C8C3bJ","outputId":"47cf84f3-bd55-4dd2-85ef-5451190fa2bb"},"outputs":[{"name":"stdout","output_type":"stream","text":["ChiSqSelector output with top 1 features selected\n","+---+------------------+-------+----------------+\n","| id|          features|clicked|selectedFeatures|\n","+---+------------------+-------+----------------+\n","|  7|[0.0,0.0,18.0,1.0]|    1.0|          [18.0]|\n","|  8|[0.0,1.0,12.0,0.0]|    0.0|          [12.0]|\n","|  9|[1.0,0.0,15.0,0.1]|    0.0|          [15.0]|\n","+---+------------------+-------+----------------+\n","\n"]}],"source":["from pyspark.ml.feature import ChiSqSelector\n","from pyspark.ml.linalg import Vectors\n","\n","df = spark.createDataFrame([\n","    (7, Vectors.dense([0.0, 0.0, 18.0, 1.0]), 1.0,),\n","    (8, Vectors.dense([0.0, 1.0, 12.0, 0.0]), 0.0,),\n","    (9, Vectors.dense([1.0, 0.0, 15.0, 0.1]), 0.0,)], [\"id\", \"features\", \"clicked\"])\n","\n","selector = ChiSqSelector(numTopFeatures=1, featuresCol=\"features\",\n","                         outputCol=\"selectedFeatures\", labelCol=\"clicked\")\n","\n","result = selector.fit(df).transform(df)\n","\n","print(\"ChiSqSelector output with top %d features selected\" % selector.getNumTopFeatures())\n","result.show()"]},{"cell_type":"markdown","metadata":{"id":"BxA-dexAC3bM"},"source":["## Reducción de la dimensioanlidad"]},{"cell_type":"markdown","metadata":{"id":"2g9wQeE0C3bN"},"source":["### PCA\n","\n","Es una técnica estadística no supervisada que se utiliza principalmente para la reducción de la dimensionalidad de las variables.Esta técnica combina las entradas de una manera específica y elimina algunas de las variables “menos importantes” manteniendo la información de las variables más importantes"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"7M2LtPOKC3bN","outputId":"6e017643-f712-4741-e812-ef82dde4a44b"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------------------------------------------------------+\n","|pcaFeatures                                                |\n","+-----------------------------------------------------------+\n","|[1.6485728230883807,-4.013282700516296,-5.524543751369388] |\n","|[-4.645104331781534,-1.1167972663619026,-5.524543751369387]|\n","|[-6.428880535676489,-5.337951427775355,-5.524543751369389] |\n","+-----------------------------------------------------------+\n","\n"]}],"source":["from pyspark.ml.feature import PCA\n","from pyspark.ml.linalg import Vectors\n","\n","data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n","        (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n","        (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n","df = spark.createDataFrame(data, [\"features\"])\n","\n","pca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\n","model = pca.fit(df)\n","\n","result = model.transform(df).select(\"pcaFeatures\")\n","result.show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"cUGpNKJrC3bN"},"source":["# Procesamiento de Texto (NLP)"]},{"cell_type":"markdown","metadata":{"id":"Q_2v2wo6C3bO"},"source":["### TF-IDF\n","Término frecuencia-frecuencia inversa de documentos (TF-IDF) es un método de vectorización de características ampliamente utilizado en la minería de texto para reflejar la importancia de un término para un documento en el corpus."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M5KQRNTwC3bO","outputId":"205cc62f-145c-4f37-8a3e-4e18a2d60e44"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----+--------------------+\n","|label|            features|\n","+-----+--------------------+\n","|  0.0|(20,[6,8,13,16],[...|\n","|  0.0|(20,[0,2,7,13,15,...|\n","|  1.0|(20,[3,4,6,11,19]...|\n","+-----+--------------------+\n","\n"]}],"source":["from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n","\n","sentenceData = spark.createDataFrame([\n","    (0.0, \"Hi I heard about Spark\"),\n","    (0.0, \"I wish Java could use case classes\"),\n","    (1.0, \"Logistic regression models are neat\")\n","], [\"label\", \"sentence\"])\n","\n","tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n","wordsData = tokenizer.transform(sentenceData)\n","\n","hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n","featurizedData = hashingTF.transform(wordsData)\n","# alternatively, CountVectorizer can also be used to get term frequency vectors\n","\n","idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n","idfModel = idf.fit(featurizedData)\n","rescaledData = idfModel.transform(featurizedData)\n","\n","rescaledData.select(\"label\", \"features\").show()"]},{"cell_type":"markdown","metadata":{"id":"qe4yHtFAC3bO"},"source":["### Word2Vec\n","\n","Word2Veces un estimador que toma secuencias de palabras que representan documentos y entrena a Word2VecModel. El modelo asigna cada palabra a un vector único de tamaño fijo. El Word2VecModel transforma cada documento en un vector utilizando el promedio de todas las palabras en el documento; este vector se puede utilizar como características para la predicción, cálculos de **similitud de documentos**, etc. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vzD0v793C3bO","outputId":"7ae678aa-ecf9-4964-a8ac-426e0fe22c76"},"outputs":[{"name":"stdout","output_type":"stream","text":["Text: [Hi, I, heard, about, Spark] => \n","Vector: [-0.03039412572979927,0.047707913815975195,0.03455201089382172]\n","\n","Text: [I, wish, Java, could, use, case, classes] => \n","Vector: [0.024677858288799013,0.0738330307815756,0.031379215951476774]\n","\n","Text: [Logistic, regression, models, are, neat] => \n","Vector: [-0.02549584247171879,0.02631833413615823,-0.02092203721404076]\n","\n"]}],"source":["from pyspark.ml.feature import Word2Vec\n","\n","# Input data: Each row is a bag of words from a sentence or document.\n","documentDF = spark.createDataFrame([\n","    (\"Hi I heard about Spark\".split(\" \"), ),\n","    (\"I wish Java could use case classes\".split(\" \"), ),\n","    (\"Logistic regression models are neat\".split(\" \"), )\n","], [\"text\"])\n","\n","# Learn a mapping from words to Vectors.\n","word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n","model = word2Vec.fit(documentDF)\n","\n","result = model.transform(documentDF)\n","for row in result.collect():\n","    text, vector = row\n","    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"]},{"cell_type":"markdown","metadata":{"id":"Mx6IvwnOC3bP"},"source":["### Tokenizer\n","\n","La tokenización es el proceso de tomar texto (como una oración) y dividirlo en términos individuales (generalmente palabras). "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"djwHDEhbC3bP","outputId":"82bf2d02-6aea-4a61-88d8-e9202403f73f"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------------------------------+------------------------------------------+------+\n","|sentence                           |words                                     |tokens|\n","+-----------------------------------+------------------------------------------+------+\n","|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n","|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n","|Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n","+-----------------------------------+------------------------------------------+------+\n","\n","+-----------------------------------+------------------------------------------+------+\n","|sentence                           |words                                     |tokens|\n","+-----------------------------------+------------------------------------------+------+\n","|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n","|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n","|Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5     |\n","+-----------------------------------+------------------------------------------+------+\n","\n"]}],"source":["from pyspark.ml.feature import Tokenizer, RegexTokenizer\n","from pyspark.sql.functions import col, udf\n","from pyspark.sql.types import IntegerType\n","\n","sentenceDataFrame = spark.createDataFrame([\n","    (0, \"Hi I heard about Spark\"),\n","    (1, \"I wish Java could use case classes\"),\n","    (2, \"Logistic,regression,models,are,neat\")\n","], [\"id\", \"sentence\"])\n","\n","tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n","\n","regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n","# alternatively, pattern=\"\\\\w+\", gaps(False)\n","\n","countTokens = udf(lambda words: len(words), IntegerType())\n","\n","tokenized = tokenizer.transform(sentenceDataFrame)\n","tokenized.select(\"sentence\", \"words\")\\\n","    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n","\n","regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n","regexTokenized.select(\"sentence\", \"words\") \\\n","    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"lySynXx-C3bQ"},"source":["### StopWordsRemover\n","\n","Las palabras vacías son palabras que deben excluirse de la entrada, generalmente porque las palabras aparecen con frecuencia y no tienen tanto significado."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zljBYIHqC3bQ","outputId":"aebda84f-1c45-4eab-d793-a8ae6a9e8718"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+----------------------------+--------------------+\n","|id |raw                         |filtered            |\n","+---+----------------------------+--------------------+\n","|0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n","|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n","+---+----------------------------+--------------------+\n","\n"]}],"source":["from pyspark.ml.feature import StopWordsRemover\n","\n","sentenceData = spark.createDataFrame([\n","    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n","    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n","], [\"id\", \"raw\"])\n","\n","remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\n","remover.transform(sentenceData).show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"6TN9cs9gC3bQ"},"source":["### n-gram\n","\n","Un n-grama es una secuencia de tokens (normalmente palabras) que aparecen con frecuencia juntos."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yG-T3a4AC3bR","outputId":"30704e6b-e5a1-4a38-ee99-3f917c9ee6b7"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------------------------------------------------+\n","|ngrams                                                            |\n","+------------------------------------------------------------------+\n","|[Hi I, I heard, heard about, about Spark]                         |\n","|[I wish, wish Java, Java could, could use, use case, case classes]|\n","|[Logistic regression, regression models, models are, are neat]    |\n","+------------------------------------------------------------------+\n","\n"]}],"source":["from pyspark.ml.feature import NGram\n","\n","wordDataFrame = spark.createDataFrame([\n","    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n","    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n","    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n","], [\"id\", \"words\"])\n","\n","ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n","\n","ngramDataFrame = ngram.transform(wordDataFrame)\n","ngramDataFrame.select(\"ngrams\").show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"FdoS5OSwC3bR"},"source":["# Funciones en SQL"]},{"cell_type":"markdown","metadata":{"id":"sghQzRsHC3bR"},"source":["### SQL Transformer"]},{"cell_type":"markdown","metadata":{"id":"X_UWFH_oC3bR"},"source":["SQLTransformer implementa las transformaciones que están definidas en SQL. Actualmente, solo admite la sintaxis SQL \"SELECT ... FROM ...__THIS__\" con WHERE, GROUP BY, etc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"duvEeyncC3bS","outputId":"29590d48-badd-4c0e-a6ae-44fd3bc723fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+---+---+---+----+\n","| id| v1| v2| v3|  v4|\n","+---+---+---+---+----+\n","|  0|1.0|3.0|4.0| 3.0|\n","|  2|2.0|5.0|7.0|10.0|\n","+---+---+---+---+----+\n","\n"]}],"source":["from pyspark.ml.feature import SQLTransformer\n","\n","df = spark.createDataFrame([\n","    (0, 1.0, 3.0),\n","    (2, 2.0, 5.0)], \n","    [\"id\", \"v1\", \"v2\"])\n","\n","sqlTrans = SQLTransformer(\n","    statement=\"SELECT *, (v1 + v2) AS v3, (v1 * v2) AS v4 FROM __THIS__\")\n","\n","sqlTrans.transform(df).show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}